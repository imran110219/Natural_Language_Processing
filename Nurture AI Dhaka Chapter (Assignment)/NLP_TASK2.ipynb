{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from python-dateutil>=2->pandas)\n",
      "Requirement already satisfied: tqdm in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already satisfied: scikit-learn in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already satisfied: gensim in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six>=1.5.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.10.0,>=1.9.2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from botocore<1.10.0,>=1.9.2->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from botocore<1.10.0,>=1.9.2->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: spacy in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-python==0.5.4 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: html5lib==1.0b8 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: six in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from spacy)\n",
      "Requirement already satisfied: termcolor in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: wcwidth in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 58.6MB/s ta 0:00:01\n",
      "\u001b[?25h  Requirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/la/anaconda3/envs/py35/lib/python3.5/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/la/anaconda3/envs/py35/lib/python3.5/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "Requirement already satisfied: numpy in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages\n"
     ]
    }
   ],
   "source": [
    "# Run this block to install dependencies [Remember to make the statement true]\n",
    "if 1 == 1:\n",
    "    !pip install pandas\n",
    "    !pip install tqdm\n",
    "    !pip install scikit-learn\n",
    "    !pip install gensim\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "# Import W2V library\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "nlp = spacy.load('en')\n",
    "DATA_LIMIT = 1000\n",
    "\n",
    "df = pd.read_csv('./imdb_master.csv', encoding='latin1')\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df = pd.concat((df_pos[:DATA_LIMIT], df_neg[:DATA_LIMIT]))\n",
    "\n",
    "def process_text(input_string, return_string=False, stem=False):\n",
    "    text = nlp(u'' + input_string)\n",
    "    if stem == True:\n",
    "        text = [tok.lemma_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    else:\n",
    "        text = [tok.lower_ for tok in text if (tok.is_alpha and not tok.is_stop)]\n",
    "    if return_string == True:\n",
    "        return \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>12500</td>\n",
       "      <td>test</td>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>12501</td>\n",
       "      <td>test</td>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10000_7.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>12502</td>\n",
       "      <td>test</td>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10001_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>12503</td>\n",
       "      <td>test</td>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10002_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>12504</td>\n",
       "      <td>test</td>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10003_8.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  type                                             review  \\\n",
       "12500       12500  test  I went and saw this movie last night after bei...   \n",
       "12501       12501  test  Actor turned director Bill Paxton follows up h...   \n",
       "12502       12502  test  As a recreational golfer with some knowledge o...   \n",
       "12503       12503  test  I saw this film in a sneak preview, and it is ...   \n",
       "12504       12504  test  Bill Paxton has taken the true story of the 19...   \n",
       "\n",
       "      label         file  \n",
       "12500   pos     0_10.txt  \n",
       "12501   pos  10000_7.txt  \n",
       "12502   pos  10001_9.txt  \n",
       "12503   pos  10002_8.txt  \n",
       "12504   pos  10003_8.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tqdm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('An', 'NN')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nlp('An example sentence. Another example sentence.')\n",
    "tokens[0].text, tokens[0].head.tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:16<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make this statement true to run from scratch [It takes time to process the text]\n",
    "if 1 == 1:\n",
    "    wordlist = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        wordlist.append(process_text(df['review'].iloc[i]))\n",
    "        \n",
    "    with open('vocabulary.txt', 'wb') as vocabulary:\n",
    "        pickle.dump(wordlist, vocabulary)\n",
    "    vocabulary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "# hs = Hierarchical Softmax, min_count -> excludes token less than that frequency, \n",
    "w2v_model = Word2Vec(window=5, workers=multiprocessing.cpu_count(), size=300, iter=100, min_count=1, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=22449, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20548792, 21977100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time w2v_model.train(wordlist, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = Dictionary(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22449"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belucci', 0.3595184087753296),\n",
       " ('romane', 0.3292483687400818),\n",
       " ('inflatable', 0.3289024829864502),\n",
       " ('dolan', 0.28767144680023193),\n",
       " ('badger', 0.27900922298431396),\n",
       " ('inverting', 0.2763495445251465),\n",
       " ('limos', 0.2744201719760895),\n",
       " ('enhancement', 0.2607278823852539),\n",
       " ('thumb', 0.2559332847595215),\n",
       " ('muriel', 0.2539374828338623)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('bellucci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cinema', 0.24760204553604126)\n",
      "('wood', 0.2246340662240982)\n",
      "('ramin', 0.21162863075733185)\n",
      "('flick', 0.2085748016834259)\n",
      "('fantsastic', 0.1930416077375412)\n",
      "('watcha', 0.1919766664505005)\n",
      "('audience', 0.18605846166610718)\n",
      "('end', 0.18570637702941895)\n",
      "('synopsis', 0.18509164452552795)\n",
      "('alternative', 0.18284578621387482)\n"
     ]
    }
   ],
   "source": [
    "for resultant in w2v_model.wv.most_similar(positive=['movie', 'good'], negative=['bad']):\n",
    "    print(resultant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create a numpy empty/random matrix with dimension of [Vocsize+1 X \n",
    "embedding dimension]\n",
    "2. Load the embeddings into that word\n",
    "3. Create keras embedding layer with the same configuration and \n",
    "load weights there\n",
    "4. Train a RNN/CNN to classify [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding = np.zeros((len(vocabulary)+1, embedding_dim))\n",
    "embedding[0] = 0\n",
    "embedding[1] = w2v_model.wv[ vocabulary[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22450, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(vocabulary)):\n",
    "    embedding[i+1] = w2v_model.wv[vocabulary[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding = np.zeros((len(vocabulary)+1, embedding_dim))\n",
    "for i in range(len(vocabulary)):\n",
    "    embedding[i + 1] = w2v_model.wv[vocabulary[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files (x86)\\python36-32\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files (x86)\\python36-32\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in c:\\program files (x86)\\python36-32\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files (x86)\\python36-32\\lib\\site-packages (from keras)\n",
      "Installing collected packages: keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\req\\req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\req\\req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\req\\req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip\\utils\\__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"c:\\program files (x86)\\python36-32\\lib\\os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [WinError 5] Access is denied: 'c:\\\\program files (x86)\\\\python36-32\\\\Lib\\\\site-packages\\\\keras'\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.6.0-cp35-cp35m-manylinux1_x86_64.whl (45.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.9MB 34kB/s  eta 0:00:01 6% |██                              | 2.9MB 6.6MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Using cached gast-0.2.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading grpcio-1.10.0-cp35-cp35m-manylinux1_x86_64.whl (7.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.5MB 181kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached astor-0.6.2-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.7.0,>=1.6.0 (from tensorflow)\n",
      "  Using cached tensorboard-1.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from tensorflow)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading absl-py-0.1.11.tar.gz (80kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from tensorflow)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Downloading protobuf-3.5.2-cp35-cp35m-manylinux1_x86_64.whl (6.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.4MB 234kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Using cached Markdown-2.6.11-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Using cached html5lib-0.9999999.tar.gz\n",
      "Requirement already satisfied: setuptools in /home/la/anaconda3/envs/py35/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Building wheels for collected packages: gast, absl-py, html5lib\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/la/.cache/pip/wheels/8e/fa/d6/77dd17d18ea23fd7b860e02623d27c1be451521af40dd4a13e\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/la/.cache/pip/wheels/3c/0f/0a/6c94612a8c26070755559045612ca3645fea91c11f2148363e\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/la/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built gast absl-py html5lib\n",
      "Installing collected packages: gast, protobuf, grpcio, astor, html5lib, bleach, markdown, tensorboard, absl-py, tensorflow\n",
      "  Found existing installation: html5lib 1.0b8\n",
      "    Uninstalling html5lib-1.0b8:\n",
      "      Successfully uninstalled html5lib-1.0b8\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed absl-py-0.1.11 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.10.0 html5lib-0.9999999 markdown-2.6.11 protobuf-3.5.2 tensorboard-1.6.0 tensorflow-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd = Embedding(len(vocabulary)+1,output_dim=embedding_dim, weights = [embedding], trainable =False, input_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(embd.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for review in wordlist:\n",
    "    X.append(np.array(vocabulary.doc2idx(review)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pad_sequences(X, value=0, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.concatenate((np.ones(1000), np.zeros(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - ETA: 1:20 - loss: 0.7267 - acc: 0.484 - ETA: 50s - loss: 0.7152 - acc: 0.480 - ETA: 38s - loss: 0.7058 - acc: 0.50 - ETA: 30s - loss: 0.7023 - acc: 0.50 - ETA: 24s - loss: 0.6943 - acc: 0.52 - ETA: 19s - loss: 0.6930 - acc: 0.52 - ETA: 15s - loss: 0.6874 - acc: 0.53 - ETA: 11s - loss: 0.6835 - acc: 0.55 - ETA: 7s - loss: 0.6794 - acc: 0.5608 - ETA: 3s - loss: 0.6765 - acc: 0.569 - 45s 32ms/step - loss: 0.6737 - acc: 0.5721 - val_loss: 0.6227 - val_acc: 0.6650\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - ETA: 29s - loss: 0.6083 - acc: 0.66 - ETA: 27s - loss: 0.5977 - acc: 0.69 - ETA: 25s - loss: 0.5849 - acc: 0.72 - ETA: 23s - loss: 0.5804 - acc: 0.73 - ETA: 19s - loss: 0.5677 - acc: 0.75 - ETA: 16s - loss: 0.5617 - acc: 0.75 - ETA: 12s - loss: 0.5648 - acc: 0.74 - ETA: 9s - loss: 0.5622 - acc: 0.7500 - ETA: 6s - loss: 0.5582 - acc: 0.750 - ETA: 2s - loss: 0.5545 - acc: 0.751 - 39s 28ms/step - loss: 0.5540 - acc: 0.7521 - val_loss: 0.5457 - val_acc: 0.7283\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - ETA: 29s - loss: 0.4466 - acc: 0.82 - ETA: 27s - loss: 0.4684 - acc: 0.80 - ETA: 24s - loss: 0.4741 - acc: 0.79 - ETA: 21s - loss: 0.4853 - acc: 0.77 - ETA: 18s - loss: 0.4757 - acc: 0.77 - ETA: 15s - loss: 0.4718 - acc: 0.77 - ETA: 12s - loss: 0.4670 - acc: 0.77 - ETA: 9s - loss: 0.4607 - acc: 0.7842 - ETA: 6s - loss: 0.4560 - acc: 0.784 - ETA: 3s - loss: 0.4571 - acc: 0.782 - 41s 29ms/step - loss: 0.4516 - acc: 0.7843 - val_loss: 0.5254 - val_acc: 0.7450\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - ETA: 32s - loss: 0.2878 - acc: 0.88 - ETA: 28s - loss: 0.3596 - acc: 0.83 - ETA: 24s - loss: 0.3659 - acc: 0.82 - ETA: 21s - loss: 0.3656 - acc: 0.83 - ETA: 18s - loss: 0.3678 - acc: 0.83 - ETA: 15s - loss: 0.3611 - acc: 0.83 - ETA: 12s - loss: 0.3627 - acc: 0.83 - ETA: 9s - loss: 0.3568 - acc: 0.8438 - ETA: 5s - loss: 0.3457 - acc: 0.846 - ETA: 2s - loss: 0.3471 - acc: 0.848 - 37s 26ms/step - loss: 0.3571 - acc: 0.8429 - val_loss: 0.5108 - val_acc: 0.7783\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - ETA: 29s - loss: 0.3001 - acc: 0.87 - ETA: 25s - loss: 0.3336 - acc: 0.86 - ETA: 22s - loss: 0.3075 - acc: 0.88 - ETA: 20s - loss: 0.2992 - acc: 0.88 - ETA: 17s - loss: 0.2863 - acc: 0.89 - ETA: 14s - loss: 0.2837 - acc: 0.89 - ETA: 11s - loss: 0.2803 - acc: 0.89 - ETA: 8s - loss: 0.2891 - acc: 0.8867 - ETA: 6s - loss: 0.2875 - acc: 0.882 - ETA: 2s - loss: 0.2903 - acc: 0.879 - 38s 27ms/step - loss: 0.2883 - acc: 0.8800 - val_loss: 0.5014 - val_acc: 0.7717\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - ETA: 27s - loss: 0.3594 - acc: 0.86 - ETA: 25s - loss: 0.3033 - acc: 0.87 - ETA: 23s - loss: 0.2664 - acc: 0.89 - ETA: 21s - loss: 0.2494 - acc: 0.89 - ETA: 17s - loss: 0.2460 - acc: 0.89 - ETA: 14s - loss: 0.2497 - acc: 0.90 - ETA: 11s - loss: 0.2401 - acc: 0.90 - ETA: 8s - loss: 0.2329 - acc: 0.9111 - ETA: 5s - loss: 0.2308 - acc: 0.912 - ETA: 2s - loss: 0.2343 - acc: 0.908 - 38s 27ms/step - loss: 0.2322 - acc: 0.9093 - val_loss: 0.4874 - val_acc: 0.7883\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - ETA: 29s - loss: 0.1939 - acc: 0.92 - ETA: 27s - loss: 0.1923 - acc: 0.92 - ETA: 25s - loss: 0.2049 - acc: 0.92 - ETA: 22s - loss: 0.2077 - acc: 0.92 - ETA: 20s - loss: 0.1974 - acc: 0.92 - ETA: 17s - loss: 0.1959 - acc: 0.92 - ETA: 13s - loss: 0.1930 - acc: 0.92 - ETA: 10s - loss: 0.1981 - acc: 0.92 - ETA: 6s - loss: 0.1950 - acc: 0.9262 - ETA: 3s - loss: 0.1976 - acc: 0.923 - 43s 31ms/step - loss: 0.1953 - acc: 0.9257 - val_loss: 0.5002 - val_acc: 0.8000\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - ETA: 37s - loss: 0.2286 - acc: 0.90 - ETA: 31s - loss: 0.1596 - acc: 0.94 - ETA: 26s - loss: 0.1628 - acc: 0.94 - ETA: 24s - loss: 0.1483 - acc: 0.95 - ETA: 21s - loss: 0.1398 - acc: 0.95 - ETA: 18s - loss: 0.1412 - acc: 0.95 - ETA: 14s - loss: 0.1362 - acc: 0.95 - ETA: 10s - loss: 0.1308 - acc: 0.95 - ETA: 7s - loss: 0.1268 - acc: 0.9618 - ETA: 3s - loss: 0.1315 - acc: 0.960 - 45s 32ms/step - loss: 0.1336 - acc: 0.9586 - val_loss: 0.5524 - val_acc: 0.8117\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - ETA: 30s - loss: 0.1218 - acc: 0.96 - ETA: 27s - loss: 0.1087 - acc: 0.96 - ETA: 25s - loss: 0.1144 - acc: 0.95 - ETA: 22s - loss: 0.1076 - acc: 0.95 - ETA: 18s - loss: 0.1119 - acc: 0.95 - ETA: 15s - loss: 0.1064 - acc: 0.96 - ETA: 12s - loss: 0.1187 - acc: 0.95 - ETA: 9s - loss: 0.1186 - acc: 0.9570 - ETA: 6s - loss: 0.1192 - acc: 0.955 - ETA: 3s - loss: 0.1159 - acc: 0.957 - 40s 28ms/step - loss: 0.1146 - acc: 0.9571 - val_loss: 0.5329 - val_acc: 0.8050\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - ETA: 29s - loss: 0.0994 - acc: 0.96 - ETA: 27s - loss: 0.1048 - acc: 0.95 - ETA: 25s - loss: 0.1080 - acc: 0.95 - ETA: 22s - loss: 0.1070 - acc: 0.95 - ETA: 19s - loss: 0.1016 - acc: 0.96 - ETA: 16s - loss: 0.0937 - acc: 0.96 - ETA: 13s - loss: 0.0924 - acc: 0.96 - ETA: 10s - loss: 0.0944 - acc: 0.96 - ETA: 6s - loss: 0.0945 - acc: 0.9688 - ETA: 3s - loss: 0.0904 - acc: 0.971 - 43s 31ms/step - loss: 0.0870 - acc: 0.9721 - val_loss: 0.5621 - val_acc: 0.8117\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - ETA: 31s - loss: 0.0908 - acc: 0.96 - ETA: 27s - loss: 0.0728 - acc: 0.97 - ETA: 24s - loss: 0.0724 - acc: 0.97 - ETA: 21s - loss: 0.0611 - acc: 0.98 - ETA: 18s - loss: 0.0575 - acc: 0.97 - ETA: 15s - loss: 0.0644 - acc: 0.97 - ETA: 12s - loss: 0.0631 - acc: 0.97 - ETA: 9s - loss: 0.0590 - acc: 0.9805 - ETA: 6s - loss: 0.0602 - acc: 0.978 - ETA: 2s - loss: 0.0605 - acc: 0.978 - 38s 27ms/step - loss: 0.0603 - acc: 0.9786 - val_loss: 0.6162 - val_acc: 0.8100\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - ETA: 30s - loss: 0.0448 - acc: 0.99 - ETA: 30s - loss: 0.0711 - acc: 0.98 - ETA: 27s - loss: 0.0580 - acc: 0.98 - ETA: 23s - loss: 0.0508 - acc: 0.98 - ETA: 19s - loss: 0.0507 - acc: 0.98 - ETA: 16s - loss: 0.0497 - acc: 0.98 - ETA: 12s - loss: 0.0459 - acc: 0.98 - ETA: 9s - loss: 0.0440 - acc: 0.9893 - ETA: 6s - loss: 0.0426 - acc: 0.990 - ETA: 2s - loss: 0.0480 - acc: 0.987 - 38s 27ms/step - loss: 0.0505 - acc: 0.9857 - val_loss: 0.6486 - val_acc: 0.8167\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - ETA: 30s - loss: 0.0404 - acc: 0.98 - ETA: 26s - loss: 0.0408 - acc: 0.98 - ETA: 24s - loss: 0.0328 - acc: 0.98 - ETA: 21s - loss: 0.0299 - acc: 0.99 - ETA: 18s - loss: 0.0271 - acc: 0.99 - ETA: 15s - loss: 0.0317 - acc: 0.99 - ETA: 12s - loss: 0.0305 - acc: 0.99 - ETA: 9s - loss: 0.0362 - acc: 0.9883 - ETA: 6s - loss: 0.0355 - acc: 0.987 - ETA: 3s - loss: 0.0353 - acc: 0.988 - 42s 30ms/step - loss: 0.0357 - acc: 0.9879 - val_loss: 0.7320 - val_acc: 0.8200\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - ETA: 34s - loss: 0.0658 - acc: 0.97 - ETA: 31s - loss: 0.0767 - acc: 0.96 - ETA: 27s - loss: 0.0648 - acc: 0.97 - ETA: 24s - loss: 0.0518 - acc: 0.97 - ETA: 20s - loss: 0.0548 - acc: 0.97 - ETA: 17s - loss: 0.0558 - acc: 0.97 - ETA: 13s - loss: 0.0587 - acc: 0.97 - ETA: 10s - loss: 0.0743 - acc: 0.97 - ETA: 6s - loss: 0.0703 - acc: 0.9748 - ETA: 3s - loss: 0.0702 - acc: 0.975 - 45s 32ms/step - loss: 0.0704 - acc: 0.9750 - val_loss: 0.7579 - val_acc: 0.8000\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - ETA: 30s - loss: 0.0178 - acc: 1.00 - ETA: 27s - loss: 0.0302 - acc: 0.99 - ETA: 24s - loss: 0.0283 - acc: 0.99 - ETA: 23s - loss: 0.0257 - acc: 0.99 - ETA: 20s - loss: 0.0278 - acc: 0.99 - ETA: 16s - loss: 0.0317 - acc: 0.99 - ETA: 13s - loss: 0.0402 - acc: 0.99 - ETA: 9s - loss: 0.0473 - acc: 0.9883 - ETA: 6s - loss: 0.0471 - acc: 0.988 - ETA: 3s - loss: 0.0467 - acc: 0.987 - 41s 29ms/step - loss: 0.0469 - acc: 0.9879 - val_loss: 0.7250 - val_acc: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211121efbe0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "from keras.layers import Bidirectional\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary) + 1, 300, weights=[embedding], trainable=False))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Bidirectional(GRU(50, dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=15, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
